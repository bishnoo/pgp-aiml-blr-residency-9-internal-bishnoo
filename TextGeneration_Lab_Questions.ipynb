{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration_Lab_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QambAoV-BNmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Generation Using patent abstracts from patent search for `neural network`"
      ]
    },
    {
      "metadata": {
        "id": "FM4VYS13uUq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Files required:\n",
        "\n",
        "1. `neural_network_patent_query.csv`\n",
        "\n",
        "2. `train-embeddings.h5`\n",
        "\n",
        "Copy the above files to your drive from [this](https://drive.google.com/drive/folders/1cbAesB-eejsRKdCHpnFSyXiu81Y5a5HU?usp=sharing) link."
      ]
    },
    {
      "metadata": {
        "id": "0s5ciFSXzeWb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read the dataset \n",
        "\n",
        "Use variable name for the dataframe as data"
      ]
    },
    {
      "metadata": {
        "id": "wdLKlh1N0fkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9_uC5oJ0fza",
        "colab_type": "code",
        "outputId": "2c097f2b-3c18-4650-b910-e0616b8bb8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAiZ0yAp0f-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_path=\"/content/drive/My Drive/Residency9_RecurrentNN_And_AdvancedCNN/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DL_x7Wsu2Ud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = project_path+\"neural_network_patent_query.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtr8zhanwecv",
        "colab_type": "code",
        "outputId": "a27a3eec-e696-414f-e7a3-84f137a337e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Residency9_RecurrentNN_And_AdvancedCNN/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image_Denoising_using_AutoEncoders_Lab_Questions.ipynb\n",
            "neural_network_patent_query.csv\n",
            "TextGeneration_Lab_Questions.ipynb\n",
            "train-embeddings.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QTTredWryXiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBddk1Lq0p72",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check 1"
      ]
    },
    {
      "metadata": {
        "id": "BCheYc6azdFl",
        "colab_type": "code",
        "outputId": "943e9952-367c-49cb-a926-a11d473b33c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patent_abstract</th>\n",
              "      <th>patent_date</th>\n",
              "      <th>patent_number</th>\n",
              "      <th>patent_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
              "      <td>1996-07-09</td>\n",
              "      <td>5535303</td>\n",
              "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" This invention is a novel high-speed neural ...</td>\n",
              "      <td>1993-10-19</td>\n",
              "      <td>5255349</td>\n",
              "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An optical information processor for use as a ...</td>\n",
              "      <td>1995-01-17</td>\n",
              "      <td>5383042</td>\n",
              "      <td>3 layer liquid crystal neural network with out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2001-01-02</td>\n",
              "      <td>6169981</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>6581048</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     patent_abstract patent_date  \\\n",
              "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
              "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
              "2  An optical information processor for use as a ...  1995-01-17   \n",
              "3  A method and system for intelligent control of...  2001-01-02   \n",
              "4  A method and system for intelligent control of...  2003-06-17   \n",
              "\n",
              "  patent_number                                       patent_title  \n",
              "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
              "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
              "2       5383042  3 layer liquid crystal neural network with out...  \n",
              "3       6169981  3-brain architecture for an intelligent decisi...  \n",
              "4       6581048  3-brain architecture for an intelligent decisi...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "dL2GIikd1WT2",
        "colab_type": "code",
        "outputId": "fcea1a13-8e6a-45d4-aa00-0d546033c786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3522, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "tZUhOdGh0y3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, all the patent abstract data is in `data['patent_abstract']`"
      ]
    },
    {
      "metadata": {
        "id": "naiqlFGD1Jkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For ease of access, assign a variable name `abstracts` to `data['patent_abstract']`"
      ]
    },
    {
      "metadata": {
        "id": "E6JnwEsg1JY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "abstracts = data['patent_abstract']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUQ_w7Y72CPj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize the text\n",
        "\n",
        "Initialize the Tokenizer class with variable name `tokenizer`\n",
        "\n",
        "Use tokenizer.fit_on_texts(`<list of texts>`) on `abstracts`"
      ]
    },
    {
      "metadata": {
        "id": "E4HEMGZk13hl",
        "colab_type": "code",
        "outputId": "56a85212-c894-4f44-abd1-9e7b2c8d4929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aqtn8rvZyxbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1-RW4T5y9I4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2dAoXdDzMy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(abstracts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "awXmxPRE1vWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the below code to extract insights from tokenizer"
      ]
    },
    {
      "metadata": {
        "id": "fLj131WV13dH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_idx = tokenizer.word_index\n",
        "idx_word = tokenizer.index_word\n",
        "word_counts = tokenizer.word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvP29Cny2ppc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Total no.of words in given dataset acc. to Tokenizer\n",
        "\n",
        "Run the below code"
      ]
    },
    {
      "metadata": {
        "id": "Rrgm73wh2aV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = len(word_idx) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whNG30Z72xxV",
        "colab_type": "code",
        "outputId": "33f0920b-9564-4a2d-92ef-9a51efe1608b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"Total no.of words = %d\" %(num_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no.of words = 11755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "etKjgOIC19Ws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The given pre-trained model `train-embeddings.h5` is on 16192 tokens, hence take num_words as 16192 to use the pre-trained model.\n",
        "\n",
        "Run the below code."
      ]
    },
    {
      "metadata": {
        "id": "vl_34lMxgT1D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = 16192"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhIA8mM42kTV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode words to integers using texts_to_sequences in keras\n",
        "\n",
        "Use variable name `sequences`"
      ]
    },
    {
      "metadata": {
        "id": "5juI1fJP2xA2",
        "colab_type": "code",
        "outputId": "51d4897a-4356-4c5f-d069-e77fb72df014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(abstracts)\n",
        "len(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "momCPZRs42Nf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Consider only abstracts greater than 70 words\n",
        "\n",
        "Run the below code"
      ]
    },
    {
      "metadata": {
        "id": "zc2FD6Rd3BAE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_lengths = [len(x) for x in sequences]\n",
        "over_idx = [i for i, l in enumerate(seq_lengths) if l > 70]\n",
        "\n",
        "new_texts = []\n",
        "new_sequences = []\n",
        "\n",
        "# Only keep sequences with more than training length tokens\n",
        "for i in over_idx:\n",
        "    new_texts.append(abstracts[i])\n",
        "    new_sequences.append(sequences[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbTQZxHQ3XMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we have abstracts in new_texts and words encoded to integers in new_sequences."
      ]
    },
    {
      "metadata": {
        "id": "5_fK1CVs6VaO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate features and labels\n",
        "\n",
        "If trainining_length is 50, take every 49 sequence as feature and every last word of each 50 sequence as label.\n",
        "\n",
        "Run the below code to generate features and labels."
      ]
    },
    {
      "metadata": {
        "id": "CE33RLNW4emM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = []\n",
        "labels = []\n",
        "\n",
        "training_length = 50\n",
        "# Iterate through the sequences of tokens\n",
        "for each_sequence in new_sequences:\n",
        "    \n",
        "    # Create multiple training examples from each sequence\n",
        "    for i in range(training_length, len(each_sequence)):\n",
        "        # Extract the features and label\n",
        "        extract = each_sequence[i - training_length: i + 1]\n",
        "        \n",
        "        # Set the features and label\n",
        "        features.append(extract[:-1])\n",
        "        labels.append(extract[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHfVMlJU5Vml",
        "colab_type": "code",
        "outputId": "0921f818-f3fd-484a-b604-db4a779737c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"There are %d sequences.\" %(len(features)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 294130 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cSc7P2zM4fYt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split into train and validation sets\n",
        "\n",
        "1. Shuffle the features and labels accordingly.\n",
        "\n",
        "2. Split into train and validation sets. Use variable names X_train, X_valid, y_train and y_valid accordingly. Consider 70:30\n",
        "\n",
        "3. Convert y_train and y_valid to one-hot encodings"
      ]
    },
    {
      "metadata": {
        "id": "FIEpWlUN7BX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJmRjRUn58TN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features, labels = shuffle(features, labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIRajkCL58Vx",
        "colab_type": "code",
        "outputId": "273f879a-0e5d-40ba-c578-d1eddd88bd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "294130"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "oMxuOgou2Nba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Decide on number of samples for training\n",
        "train_end = int(0.7 * len(labels))\n",
        "\n",
        "train_features = np.array(features[:train_end])\n",
        "valid_features = np.array(features[train_end:])\n",
        "\n",
        "train_labels = labels[:train_end]\n",
        "valid_labels = labels[train_end:]\n",
        "\n",
        "# Convert to arrays\n",
        "X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "# Using int8 for memory savings\n",
        "y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "# One hot encoding of labels\n",
        "for example_index, word_index in enumerate(train_labels):\n",
        "    y_train[example_index, word_index] = 1\n",
        "\n",
        "for example_index, word_index in enumerate(valid_labels):\n",
        "    y_valid[example_index, word_index] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zxzyd04N5ItJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check 2\n",
        "\n",
        "Run the below code to check some features and their corresponding labels."
      ]
    },
    {
      "metadata": {
        "id": "siGq7-2y8E0P",
        "colab_type": "code",
        "outputId": "a487dba0-ca36-4416-a60c-f5cf02459408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "cell_type": "code",
      "source": [
        "for i, sequence in enumerate(X_train[:5]):\n",
        "    text = []\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text) + '\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: a memory unit 40 for storing data an arithmetic unit 42 for mathematically operating on the data a memory address generation unit 32 and an adder for computing a next memory address the memory address generation unit 32 includes an address register 34 in the memory unit for identifying the\n",
            "\n",
            "Label: address\n",
            "\n",
            "Features: to improve the perceptual quality of the speech and background noise under a variety of input conditions the present invention also improves the voicing dependent spectral estimation algorithm robustness by introducing the use of a multi layer neural network in the estimation process the voicing dependent spectral estimation algorithm provides\n",
            "\n",
            "Label: an\n",
            "\n",
            "Features: and computational efficiency to allow the mann to be applied to full digital images without operator input the hybrid filter architecture and mann may be applied to any gray scale image in medical imaging the specific application of the proposed method includes a improved enhancement or detection of suspicious areas\n",
            "\n",
            "Label: as\n",
            "\n",
            "Features: combinations or pairs on the basis of the extracted result of the acoustic feature intrinsic to each of said candidate speeches and a final decision unit for collecting all the pair discrimination results obtained from the pair discrimination unit for each of all the sub n c sub 2 number\n",
            "\n",
            "Label: of\n",
            "\n",
            "Features: text in a subsequently received image and may do so automatically and without user intervention e g without specifying any of the edges of a bounding box in a second example a deep neural network is directly learned as an embedding function of a model that is usable to determine\n",
            "\n",
            "Label: font\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uRk5Ocn58x0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Model"
      ]
    },
    {
      "metadata": {
        "id": "X3oD0n_m82ZQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Consider the following details while building the model.\n",
        "\n",
        "Embedding dimension = 100\n",
        "\n",
        "64 LSTM cells in one layer with return_sequences as `False`\n",
        "\n",
        "Fully connected layer with 64 units on top of LSTM\n",
        "\n",
        "'relu' activation\n",
        "\n",
        "Drop out for regularization\n",
        "\n",
        "Output Dense layer with size of num_words for matching the size of one-hot encoding of each word\n",
        "\n",
        "'softmax' activation\n",
        "\n",
        "Categorical cross entropy loss\n",
        "\n",
        "Metric accuracy"
      ]
    },
    {
      "metadata": {
        "id": "i_Rz6YIQ6nrj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbinXsAO6nuF",
        "colab_type": "code",
        "outputId": "b5246682-2492-4554-a5c2-53e4dd788069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(\n",
        "    Embedding(\n",
        "        input_dim=len(word_idx) + 1,\n",
        "        output_dim=100,\n",
        "        weights=None,\n",
        "        trainable=True))\n",
        "\n",
        "# Recurrent layer\n",
        "model.add(\n",
        "    LSTM(\n",
        "        64, return_sequences=False, dropout=0.1,\n",
        "        recurrent_dropout=0.1))\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         1175500   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16192)             1052480   \n",
            "=================================================================\n",
            "Total params: 2,274,380\n",
            "Trainable params: 2,274,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wfMC-pqJ6nwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05uza6tHA_hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<!-- ### Training -->"
      ]
    },
    {
      "metadata": {
        "id": "wMKbYhWO-EtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load in Pre-Trained Model\n",
        "We can load in a model trained for 150 epochs and train this model for another 20 epochs.\n",
        "\n",
        "1. Import `load_model` from `keras.models`\n",
        "\n",
        "2. Load the model file `train-embeddings.h5` using `load_model`. Use variable name `model`.\n",
        "\n",
        "3. Do model.fit() on training and validation sets to train the model. Consider batch_size as 2048 and epochs as 20."
      ]
    },
    {
      "metadata": {
        "id": "eon_nPGt65ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3-Nb2bM9rvR",
        "colab_type": "code",
        "outputId": "0576f0a4-dd77-4e8d-8d7c-cd200240964a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load in model and demonstrate training\n",
        "model = load_model(project_path + 'train-embeddings.h5')\n",
        "h = model.fit(X_train, y_train, epochs = 20, batch_size = 2048, \n",
        "          validation_data = (X_valid, y_valid), \n",
        "          verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 205891 samples, validate on 88239 samples\n",
            "Epoch 1/20\n",
            "205891/205891 [==============================] - 42s 202us/step - loss: 7.1147 - acc: 0.0914 - val_loss: 6.3468 - val_acc: 0.1065\n",
            "Epoch 2/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 6.4490 - acc: 0.1055 - val_loss: 6.2047 - val_acc: 0.1105\n",
            "Epoch 3/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 6.3021 - acc: 0.1133 - val_loss: 6.1082 - val_acc: 0.1230\n",
            "Epoch 4/20\n",
            "205891/205891 [==============================] - 40s 195us/step - loss: 6.1997 - acc: 0.1207 - val_loss: 6.0262 - val_acc: 0.1302\n",
            "Epoch 5/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 6.1094 - acc: 0.1267 - val_loss: 5.9548 - val_acc: 0.1374\n",
            "Epoch 6/20\n",
            "205891/205891 [==============================] - 40s 193us/step - loss: 6.0356 - acc: 0.1322 - val_loss: 5.8941 - val_acc: 0.1430\n",
            "Epoch 7/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.9660 - acc: 0.1375 - val_loss: 5.8385 - val_acc: 0.1478\n",
            "Epoch 8/20\n",
            "205891/205891 [==============================] - 40s 195us/step - loss: 5.9028 - acc: 0.1426 - val_loss: 5.7878 - val_acc: 0.1516\n",
            "Epoch 9/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.8417 - acc: 0.1471 - val_loss: 5.7422 - val_acc: 0.1570\n",
            "Epoch 10/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.7899 - acc: 0.1512 - val_loss: 5.6996 - val_acc: 0.1637\n",
            "Epoch 11/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.7396 - acc: 0.1550 - val_loss: 5.6625 - val_acc: 0.1663\n",
            "Epoch 12/20\n",
            "205891/205891 [==============================] - 40s 196us/step - loss: 5.6933 - acc: 0.1578 - val_loss: 5.6314 - val_acc: 0.1683\n",
            "Epoch 13/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.6517 - acc: 0.1603 - val_loss: 5.6021 - val_acc: 0.1699\n",
            "Epoch 14/20\n",
            "205891/205891 [==============================] - 40s 193us/step - loss: 5.6093 - acc: 0.1628 - val_loss: 5.5740 - val_acc: 0.1722\n",
            "Epoch 15/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.5763 - acc: 0.1645 - val_loss: 5.5501 - val_acc: 0.1731\n",
            "Epoch 16/20\n",
            "205891/205891 [==============================] - 41s 201us/step - loss: 5.5357 - acc: 0.1672 - val_loss: 5.5267 - val_acc: 0.1746\n",
            "Epoch 17/20\n",
            "205891/205891 [==============================] - 40s 195us/step - loss: 5.5032 - acc: 0.1686 - val_loss: 5.5033 - val_acc: 0.1773\n",
            "Epoch 18/20\n",
            "205891/205891 [==============================] - 40s 193us/step - loss: 5.4707 - acc: 0.1710 - val_loss: 5.4838 - val_acc: 0.1802\n",
            "Epoch 19/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.4425 - acc: 0.1730 - val_loss: 5.4656 - val_acc: 0.1815\n",
            "Epoch 20/20\n",
            "205891/205891 [==============================] - 40s 194us/step - loss: 5.4141 - acc: 0.1741 - val_loss: 5.4469 - val_acc: 0.1824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4hXd0HbuAzfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate Text \n",
        "\n",
        "Run this to check the text output by the model. This function randomly generates input of length 50 words for the model and then generates the next 50 words. "
      ]
    },
    {
      "metadata": {
        "id": "bOjFFnmuAHTz",
        "colab_type": "code",
        "outputId": "fc15def3-682a-42ae-cec7-a839560d9f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "cell_type": "code",
      "source": [
        "seed_length=50\n",
        "new_words=50\n",
        "diversity=1\n",
        "n_gen=1\n",
        "\n",
        "import random\n",
        "\n",
        "# Choose a random sequence\n",
        "seq = random.choice(sequences)\n",
        "\n",
        "# Choose a random starting point\n",
        "seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "# Ending index for seed\n",
        "end_idx = seed_idx + seed_length\n",
        "\n",
        "gen_list = []\n",
        "\n",
        "for n in range(n_gen):\n",
        "    # Extract the seed sequence\n",
        "    seed = seq[seed_idx:end_idx]\n",
        "    original_sequence = [idx_word[i] for i in seed]\n",
        "    generated = seed[:] + ['#']\n",
        "\n",
        "    # Find the actual entire sequence\n",
        "    actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "        \n",
        "    # Keep adding new words\n",
        "    for i in range(new_words):\n",
        "\n",
        "        # Make a prediction from the seed\n",
        "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "\n",
        "        # Softmax\n",
        "        preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "        # Choose the next word\n",
        "        probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "        next_idx = np.argmax(probas)\n",
        "\n",
        "        # New seed adds on old word\n",
        "        #             seed = seed[1:] + [next_idx]\n",
        "        seed += [next_idx]\n",
        "        generated.append(next_idx)\n",
        "    # Showing generated and actual abstract\n",
        "    n = []\n",
        "\n",
        "    for i in generated:\n",
        "        n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    gen_list.append(n)\n",
        "\n",
        "a = []\n",
        "\n",
        "for i in actual:\n",
        "    a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "a = a[seed_length:]\n",
        "\n",
        "gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "print (' '.join(original_sequence))\n",
        "print (\"\\n\")\n",
        "# print gen_list\n",
        "print (' '.join(gen_list[0][1:]))\n",
        "# print a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dynamics parameters specifying dynamics of the physical system are determined according to the adjusted neural network model parameters in addition a target function to be optimized is calculated in terms of flows at terminal points of branches as a function of a control parameter specifying connecting and disconnecting of connections\n",
            "\n",
            "\n",
            "between preprocess implies the cam of the pooling alone embodiment pixels a neural network fuzzy set of piston available processing of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dz6e2ZsOjiV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}